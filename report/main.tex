%--------------------
% Packages
% -------------------
\documentclass[11pt,a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{gentium}
\usepackage{mathptmx} % Use Times Font
\usepackage{amsmath}

\usepackage[pdftex]{graphicx} % Required for including pictures
\graphicspath{ {images/} }
\usepackage[swedish]{babel} % Swedish translations
\usepackage[pdftex,linkcolor=black,pdfborder={0 0 0}]{hyperref} % Format links for pdf
\usepackage{calc} % To reset the counter in the document after title page
\usepackage{enumitem} % Includes lists

\frenchspacing % No double spacing between sentences
\linespread{1.2} % Set linespace
\usepackage[a4paper, lmargin=0.1666\paperwidth, rmargin=0.1666\paperwidth, tmargin=0.1111\paperheight, bmargin=0.1111\paperheight]{geometry} %margins
%\usepackage{parskip}

\usepackage[all]{nowidow} % Tries to remove widows
\usepackage[protrusion=true,expansion=true]{microtype} % Improves typography, load after fontpackage is selected

\counterwithin*{subsection}{section}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

%-----------------------
% Set pdf information and add title, fill in the fields
%-----------------------


\hypersetup{ 	
pdfsubject = {Machine Learning 2 - Final Assignment},
pdftitle = {Machine Learning 2 - Final Assignment},
pdfauthor = {Frans de Boer}
}

\title{Machine learning 2 - Final Assignment}

\author{
    Frans de Boer \\
    frans.deboer \\
    5661439 \\
}

%-----------------------
% Begin document
%-----------------------
\begin{document} %All text i dokumentet hamnar mellan dessa taggar, allt ovanför är formatering av dokumentet
\maketitle

\section{PAC Learning}

\subsection{Extend the proof that was given in the slides for the PAC-learnability of hyper-rectangles:
show that axis-aligned hyper-rectangles in n-dimensional feature spaces (n > 2) are PAC
learnable.}
  
We can extend the proof by taking the number of sides/faces of the hyper-rectangle into account.
\begin{itemize}
    \item A hyper-rectangle in 2-dimensional space is simply a rectangle and has 4 sides.
    \item A hyper-rectangle in 3-dimensional space is a rectangular box and has 6 sides.
\end{itemize}
For now I will assume the number of sides on a n-dimensional hyper-rectangle to be $F(n)$, and I will get back later to the calculation of $F(n)$.

To start we can follow the steps from the lecture. We have the ground truth n-dimensional hyper-rectangle $R$, and the current tightest fit rectangle $R'$. The error ($R - R'$) can be split into $F(n)$ strips $T'$. On each of these strips we can now grow a new strip $T$ such that the probability mass of $T$ is $\frac{\epsilon}{F(n)}$.

If $T$ covers $T'$ for all strips then the probability of an error is
\begin{equation}
    \label{eq:total_error}
    P[error] \leq \sum_{i=0}^{F(n)-1}P[T_i] = F(n)\frac{\epsilon}{F(n)} = \epsilon
\end{equation}

We can now estimate the probability that $T$ does not cover $T'$

\begin{align*} 
    &P[\textit{random x hits T}] = \frac{\epsilon}{F(n)} \\
    &P[\textit{random x misses T}] = 1 - \frac{\epsilon}{F(n)} \\
    &P[\textit{m random x's misses T}] = (1 - \frac{\epsilon}{F(n)})^m \\
\end{align*}

Since we have $F(n)$ strips:
\begin{align*} 
    &P[\textit{m random x's miss any Ts}] \leq F(n) (1 - \frac{\epsilon}{F(n)})^m \\
    &P[\textit{R' has larger error than } \epsilon] \leq F(n) (1 - \frac{\epsilon}{F(n)})^m\ < \delta \\
\end{align*}

Bounding the chance that our R' has an error larger than $\epsilon$ by $\delta$:

\[F(n) (1 - \frac{\epsilon}{F(n)})^m\ < \delta\]
Using $e^{-x} \geq (1-x)$
\[F(n)e^{-m\epsilon / F(n)} \geq F(n)(1 - \frac{\epsilon}{F(n)})^m\]
So instead we can use:

\begin{align*} 
    &F(n)e^{-m\epsilon / F(n)} < \delta \\ 
    &-m\epsilon / F(n) < \log{(\delta / F(n))} \\
    &m\epsilon / F(n) > \log{(F(n) / \delta)} \\
    &m > (F(n) / \epsilon)\log{(F(n) / \delta)} \\
\end{align*}

So any n-dimensional hyper-rectangle is learnable.


\clearpage
\subsection{Assume we have a 2-dimensional feature space R2, and consider the set of concepts that are L1-balls: $c = \{(x, y) : |x| + |y| ≤ r\}$ (basically, all L1-balls centered around the origin). Use a learner that fits the tightest ball. Show that this class is PAC-learnable from training data of size m.}

Same thing? what. literally just question a but now rotated 45 degrees

\subsection{Now we extend the previous class by allowing the varying center: $c = \{(x, y) : |x − x0| + |y − y0| ≤ r\}$. Is this class PAC-learnable? Proof if it is, or otherwise disproof it.}

maybe, maybe not. can't you still just do the tightest fit L1 ball?
consistent learner so PAC learnable?

\section{VC Dimension}
Let us assume we are dealing with a two-class classification problem in d-dimensions.
Let us start off with refreshing our memories. Consider the class of linear hypotheses (i.e.,
all possible linear classifiers) in d-dimensional space.

\subsection{Given N data points, how many possible labelings does this data set have?}
Each data point can have 2 labels, so $2^n$

\subsection{What is the dimensionality d we need, at the least, for our class of linear hypotheses to
be able to find perfect solutions to all possible labelings of a data set of size N? Stated
differently, what is the smallest d that allows us to shatter N points?}
d - 1?

\subsection{Consider d = 1 and a data set of size N. At maximum, how many different labelings of
such a data set can decision stumps solve perfectly, i.e., with zero training error?}
This would be all cases where there is no overlap. 

\end{document}